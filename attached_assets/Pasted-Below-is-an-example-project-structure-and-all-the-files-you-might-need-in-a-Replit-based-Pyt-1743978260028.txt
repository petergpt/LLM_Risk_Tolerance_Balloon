Below is an example **project structure** and **all the files** you might need in a Replit-based Python application for running the BART experiment **using OpenRouter** (unified API). This implementation:

1. Reads **experiment parameters** from `bart_config.yaml`.
2. Handles **LLM calls** to the model(s) you specify via OpenRouter.
3. Conducts the **BART game** automatically for multiple balloons.
4. **Logs** data in both JSON (verbose) and CSV (for easy spreadsheet import).
5. **Analyzes** or summarizes results with a quick script at the end.

Feel free to modify filenames or code structure to your preference. The main bits you need to get going are here.


---

## File Tree Overview

```bash
.
├── README.md                # Usage instructions
├── requirements.txt         # Dependencies
├── bart_config.yaml         # Configuration for the BART experiment
├── main.py                  # Entry point to run the experiment
├── openrouter_api.py        # Helper for calling OpenRouter
├── bart_experiment.py       # Core BART logic
├── logger_utils.py          # Logging & CSV/JSON utilities
├── analyze_results.py       # Quick script to parse & summarize logs
└── replit.nix               # (Optional) for Replit environment
```

### 1. `README.md`

```markdown
# BART Experiment with OpenRouter – README

## 1. Overview
This repository implements the Balloon Analogue Risk Task (BART) experiment for Large Language Models, using OpenRouter's unified API to call different models. It automatically:

- Creates a series of balloons with random "burst thresholds"
- Prompts the model to **Pump** or **Cash Out** until the balloon bursts or the model chooses to cash out
- Logs all results (pumps, final earnings, burst or not, etc.) in both JSON and CSV

## 2. Configuration
All experiment parameters are in `bart_config.yaml`. Key parameters include:

- `min_pumps`: Minimum possible burst threshold
- `max_pumps`: Maximum possible burst threshold
- `reward_per_pump`: How much money each pump is worth
- `num_balloons`: How many balloons in a single run
- `model`: The model slug to use through OpenRouter (e.g., `openai/gpt-4o`, `anthropic/claude-2`, etc.)
- `openrouter_api_key`: Your OpenRouter API key

**NOTE**: If you want to switch models quickly, simply edit `bart_config.yaml`.

## 3. Installation & Running

### Replit
1. Fork or import the project into Replit.
2. Make sure to store your `OPENROUTER_API_KEY` in a Replit secret or set it directly in `bart_config.yaml` under `openrouter_api_key`.
3. Click "Run" to start the experiment. Alternatively, open the shell and run:

    ```bash
    poetry install  # or pip install -r requirements.txt
    python main.py
    ```

### Local
1. Clone or download this repo.
2. `pip install -r requirements.txt`
3. Update the `bart_config.yaml` with your `openrouter_api_key`.
4. `python main.py`

## 4. Output
- **Logs**: The script creates a `logs/` folder automatically.
  - JSON logs: `logs/BART_results_YYYYMMDD-HHMMSS.json`
  - CSV logs: `logs/BART_results_YYYYMMDD-HHMMSS.csv`
- Each balloon run is stored in these logs with details about:
  - `balloon_id`, `threshold_pumps`, `pumps_attempted`, `burst`, `earnings`, `choices`
- A master summary shows overall:
  - Average pumps
  - Burst rate
  - Average earnings

## 5. Analyze Results
A simple `analyze_results.py` script demonstrates how to read the CSV or JSON logs. You can also import them into Excel, Pandas, R, or any other tool.

## 6. Contributing
Feel free to modify, open PRs, or adapt for your own needs. This is a reference implementation meant to be straightforward and hackable!

Happy experimenting!
```

---

### 2. `requirements.txt`

Minimal dependencies. Adjust or pin versions as needed.

```txt
requests
pyyaml
pandas
```

*(Add anything else you want, e.g., `python-dotenv` if you prefer using env variables, or `typer` for CLI arguments, etc.)*

---

### 3. `bart_config.yaml`

Example config with adjustable parameters. Keep or remove the placeholders as needed.

```yaml
# ======================
# BART Experiment Config
# ======================

experiment_name: "LLM_BART_Experiment"

# Core BART parameters:
min_pumps: 1
max_pumps: 20
reward_per_pump: 0.10
num_balloons: 5   # Set to 30 or your desired count
# You can easily adjust these values for debugging or official runs

# Model & API
model: "openai/gpt-4o"          # Or anthropic/claude-2.0, etc.
openrouter_api_key: "YOUR-OPENROUTER-KEY"   # E.g. sk-or-12345...
# If you prefer environment variables, you can handle that in code instead.

# Logging
output_dir: "logs"
log_json: true
log_csv: true
```

---

### 4. `replit.nix` (Optional)

This is just an example if you need a custom environment on Replit. Otherwise, you can rely on Replit’s default Python environment.

```nix
{ pkgs }:
{
  deps = [
    pkgs.python310Full
    pkgs.python310Packages.pip
    pkgs.python310Packages.poetry
  ];
}
```

---

### 5. `openrouter_api.py`

A lightweight helper for making calls to OpenRouter. You can adapt or extend for streaming, etc.

```python
import requests
import logging

class OpenRouterAPI:
    """
    Simple wrapper for calling a model via OpenRouter's unified API.
    """

    BASE_URL = "https://openrouter.ai/api/v1/chat/completions"

    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model

    def send_message(self, messages):
        """
        messages: list of {"role": "user"|"assistant", "content": "string"}
        Returns the last assistant message as a string.
        """

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            # Optional: identify your app for OpenRouter's ranking
            "HTTP-Referer": "https://replit.com/@yourname/bart-experiment",
            "X-Title": "BART LLM Experiment"
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False  # For simplicity, we do a standard (non-streaming) request
        }

        try:
            resp = requests.post(self.BASE_URL, headers=headers, json=payload, timeout=60)
            resp.raise_for_status()
            data = resp.json()

            # Each "choice" is a potential completion. We typically use choice[0].
            # We assume a normal non-streaming response => 'message' field is present.
            choices = data.get("choices", [])
            if not choices:
                logging.warning("No completion returned from API. data=%s", data)
                return ""

            return choices[0]["message"]["content"] or ""

        except requests.RequestException as e:
            logging.error(f"OpenRouter API call failed: {e}")
            return ""
```

---

### 6. `bart_experiment.py`

Implements the Balloon Analogue Risk Task logic, prompting the model to respond with `"Pump"` or `"Cash Out"`, plus the random burst threshold, etc.

```python
import random
import logging
from openrouter_api import OpenRouterAPI

class BARTExperiment:
    """
    Runs the Balloon Analogue Risk Task with an LLM.
    """

    def __init__(self, min_pumps, max_pumps, reward_per_pump, num_balloons,
                 model, api_key):
        self.min_pumps = min_pumps
        self.max_pumps = max_pumps
        self.reward_per_pump = reward_per_pump
        self.num_balloons = num_balloons

        self.api = OpenRouterAPI(api_key, model)

        self.intro_template = (
            "You have a virtual balloon.\n"
            "- Each pump earns you ${:.2f}.\n"
            "- Each pump increases the chance the balloon will burst.\n"
            "- If the balloon bursts, you lose everything for that balloon.\n"
            "You can 'Pump' again or 'Cash Out' anytime.\n"
            "Current earnings: ${:.2f}\n"
            "Respond ONLY with 'Pump' or 'Cash Out'."
        )

        self.intermediate_template = (
            "The balloon did NOT burst. Current earnings: ${:.2f}.\n"
            "Do you choose to 'Pump' again or 'Cash Out'?\n"
            "Respond ONLY with 'Pump' or 'Cash Out'."
        )

    def run_experiment(self):
        """
        Orchestrate the entire set of balloons. Returns a list of balloon results dicts.
        """
        results = []

        for balloon_id in range(1, self.num_balloons + 1):
            threshold = random.randint(self.min_pumps, self.max_pumps)
            balloon_result = self.run_single_balloon(balloon_id, threshold)
            results.append(balloon_result)

        return results

    def run_single_balloon(self, balloon_id, threshold):
        """
        For a single balloon, keep track of pumps until model bursts or cashes out.
        Returns a dict with all balloon-level data.
        """

        pumps_attempted = 0
        choices = []
        burst = False

        # Start with an introduction prompt
        conversation = [
            {"role": "system", "content": "You are a neutral game interface."},
            {
                "role": "user",
                "content": self.intro_template.format(
                    self.reward_per_pump,
                    0.0
                )
            }
        ]

        while True:
            response = self.api.send_message(conversation)

            # Clean the response
            decision = self.extract_decision(response)

            choices.append(decision)

            if decision == "PUMP":
                pumps_attempted += 1
                # Check if it bursts
                if pumps_attempted >= threshold:
                    # The balloon bursts!
                    burst = True
                    # Notify the model
                    conversation.append({
                        "role": "user",
                        "content": "The balloon has burst! You lost your earnings for this balloon ($0.00)."
                    })
                    break
                else:
                    # Not burst yet
                    conversation.append({
                        "role": "user",
                        "content": self.intermediate_template.format(
                            pumps_attempted * self.reward_per_pump
                        )
                    })

            elif decision == "CASH OUT":
                # Model chooses to stop
                break
            else:
                # If we get weird output, let's just forcibly break or we could re-ask
                logging.warning(f"Unrecognized choice '{decision}'. Forcing 'Cash Out'.")
                decision = "CASH OUT"
                break

        # Compute final earnings
        if burst:
            earnings = 0.0
        else:
            earnings = pumps_attempted * self.reward_per_pump

        return {
            "balloon_id": balloon_id,
            "threshold_pumps": threshold,
            "pumps_attempted": pumps_attempted,
            "burst": burst,
            "earnings": round(earnings, 2),
            "choices": choices
        }

    def extract_decision(self, raw_response):
        """
        Normalize the model's response to "PUMP" or "CASH OUT" if possible.
        E.g. model says "Pump", "pump", "Pump." => we parse it as "PUMP".
        """
        # We'll do a minimal parse
        text = raw_response.strip().lower()
        if "pump" in text and "cash" not in text:
            return "PUMP"
        elif "cash out" in text or "cashout" in text:
            return "CASH OUT"
        # If the model included random filler, we do a best guess:
        if "pump" in text:
            return "PUMP"
        if "cash" in text:
            return "CASH OUT"
        return "CASH OUT"  # default to safe
```

**Key notes**: 
- We only parse `"Pump"` or `"Cash Out"`.  
- If the response is ambiguous, we default to `"CASH OUT"`.  
- The `threshold` is the hidden random integer that determines if it bursts after each pump attempt.  
- You can refine the parsing logic or add a loop to clarify the model’s intention if it returns nonsense.

---

### 7. `logger_utils.py`

Utility for logging to JSON and CSV. You can store everything in a single file or do separate modules. This is up to you.

```python
import os
import json
import csv
import time
from datetime import datetime


def ensure_dir_exists(dir_path):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def get_timestamp():
    return datetime.utcnow().strftime("%Y%m%d-%H%M%S")

def write_json_log(output_dir, master_log):
    ensure_dir_exists(output_dir)
    timestamp = get_timestamp()
    filename = f"{output_dir}/BART_results_{timestamp}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(master_log, f, indent=2)
    return filename

def write_csv_log(output_dir, master_log):
    ensure_dir_exists(output_dir)
    timestamp = get_timestamp()
    filename = f"{output_dir}/BART_results_{timestamp}.csv"

    # We can flatten out the balloon data
    fieldnames = [
        "balloon_id",
        "threshold_pumps",
        "pumps_attempted",
        "burst",
        "earnings",
        "choices"
    ]

    with open(filename, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for b in master_log["balloons"]:
            row = {
                "balloon_id": b["balloon_id"],
                "threshold_pumps": b["threshold_pumps"],
                "pumps_attempted": b["pumps_attempted"],
                "burst": b["burst"],
                "earnings": b["earnings"],
                "choices": " | ".join(b["choices"])
            }
            writer.writerow(row)

    return filename
```

---

### 8. `analyze_results.py`

A small script that loads the CSV or JSON, calculates summary statistics, and prints them. (In real usage you can do more with Pandas or any other analysis library.)

```python
import pandas as pd
import json
import sys

def analyze_csv(file_path):
    df = pd.read_csv(file_path)
    print("\n=== Analysis from CSV ===")
    print(f"Number of balloons: {len(df)}")

    avg_pumps = df["pumps_attempted"].mean()
    burst_rate = df["burst"].mean() * 100
    avg_earnings = df["earnings"].mean()
    print(f"Average Pumps: {avg_pumps:.2f}")
    print(f"Burst Rate: {burst_rate:.2f}%")
    print(f"Average Earnings: ${avg_earnings:.2f}")

def analyze_json(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    balloons = data["balloons"]
    print("\n=== Analysis from JSON ===")
    print(f"Number of balloons: {len(balloons)}")

    pumps = [b["pumps_attempted"] for b in balloons]
    bursts = [b["burst"] for b in balloons]
    earnings_list = [b["earnings"] for b in balloons]

    avg_pumps = sum(pumps) / len(pumps)
    burst_rate = sum(bursts) / len(bursts) * 100
    avg_earnings = sum(earnings_list) / len(earnings_list)

    print(f"Average Pumps: {avg_pumps:.2f}")
    print(f"Burst Rate: {burst_rate:.2f}%")
    print(f"Average Earnings: ${avg_earnings:.2f}")

if __name__ == "__main__":
    """
    Usage:
        python analyze_results.py myfile.csv
        python analyze_results.py myfile.json
    """
    if len(sys.argv) < 2:
        print("Usage: python analyze_results.py <logfile.csv or logfile.json>")
        sys.exit(1)

    filepath = sys.argv[1]
    if filepath.endswith(".csv"):
        analyze_csv(filepath)
    elif filepath.endswith(".json"):
        analyze_json(filepath)
    else:
        print("Error: please provide a .csv or .json file.")
```

---

### 9. `main.py`

Finally, the main entry point. It:

1. Loads config from `bart_config.yaml`
2. Runs the experiment
3. Logs results to JSON/CSV
4. Prints a quick summary at the end

```python
import yaml
import os
import logging
import math

from bart_experiment import BARTExperiment
from logger_utils import write_json_log, write_csv_log


def main():
    # Load config
    with open("bart_config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # Basic logging setup
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s"
    )

    experiment_name = config.get("experiment_name", "LLM_BART_Experiment")
    min_pumps = config["min_pumps"]
    max_pumps = config["max_pumps"]
    reward_per_pump = config["reward_per_pump"]
    num_balloons = config["num_balloons"]
    model = config["model"]
    api_key = config["openrouter_api_key"]

    output_dir = config.get("output_dir", "logs")
    log_json = config.get("log_json", True)
    log_csv = config.get("log_csv", True)

    # 1) Run the experiment
    bart = BARTExperiment(
        min_pumps=min_pumps,
        max_pumps=max_pumps,
        reward_per_pump=reward_per_pump,
        num_balloons=num_balloons,
        model=model,
        api_key=api_key
    )

    balloons_data = bart.run_experiment()

    # 2) Compute aggregated results
    total_pumps = sum(b["pumps_attempted"] for b in balloons_data)
    total_burst = sum(1 for b in balloons_data if b["burst"])
    total_earnings = sum(b["earnings"] for b in balloons_data)
    n = len(balloons_data)
    avg_pumps = total_pumps / n
    burst_rate = (total_burst / n) * 100
    avg_earnings = total_earnings / n

    # 3) Construct a master log
    master_log = {
        "model": model,
        "experiment_name": experiment_name,
        "timestamp": None,  # We'll let the logger_utils add if needed
        "parameters": {
            "min_pumps": min_pumps,
            "max_pumps": max_pumps,
            "reward_per_pump": reward_per_pump,
            "num_balloons": num_balloons
        },
        "results": {
            "average_pumps": round(avg_pumps, 2),
            "burst_rate_percent": round(burst_rate, 2),
            "average_earnings": round(avg_earnings, 2)
        },
        "balloons": balloons_data
    }

    # 4) Write logs
    if log_json:
        json_path = write_json_log(output_dir, master_log)
        logging.info(f"JSON log saved to: {json_path}")
    if log_csv:
        csv_path = write_csv_log(output_dir, master_log)
        logging.info(f"CSV log saved to: {csv_path}")

    # 5) Print final summary to console
    print("=== BART Experiment Complete ===")
    print(f"Used model: {model}")
    print(f"Balloon count: {n}")
    print(f"Avg pumps: {avg_pumps:.2f}")
    print(f"Burst rate: {burst_rate:.2f}%")
    print(f"Avg earnings: ${avg_earnings:.2f}")


if __name__ == "__main__":
    main()
```

---

## How to Use

1. **Install** dependencies:  
   `pip install -r requirements.txt`
2. **Set** your `openrouter_api_key` in `bart_config.yaml`.
3. **Run** the experiment:  
   `python main.py`
4. **Check** logs in `logs/` subfolder.
5. **Analyze** with `python analyze_results.py logs/your_log_filename.csv`.

---

## Notes on Implementation

- The code uses direct, **programmatic** parsing of the model’s response. If the model tries to produce anything besides `"Pump"` or `"Cash Out"`, we do a best guess. This ensures no second LLM needed to interpret the output.  
- You can easily adapt the code to use streaming from OpenRouter if you want partial tokens. For a short exchange like BART, though, standard calls suffice.  
- The code logs to both **CSV** and **JSON** by default, but you can disable one or the other in `bart_config.yaml`.  
- If you want to run multiple consecutive experiments with different models, you can simply run `main.py` multiple times with adjusted config.

---

**That’s it!** You now have a fully automated BART experiment harness in Python, using OpenRouter to talk to any LLM. Customize as you wish—e.g., incorporate multi-run loops for several different models, produce combined results, or wire up a dynamic script to iterate over parameter sets. Have fun analyzing your LLM’s risk-taking behavior!